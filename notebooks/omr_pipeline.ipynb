{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmonia Occulta: Main Analysis Notebook\n",
    "\n",
    "This notebook reproduces the core results from the paper:\n",
    "\n",
    "  Morgan H. Sherer,\n",
    "  \"The Completed Harmony: A Computational Pilot Study of Musical Encodings\n",
    "   in Robert Fludd's Monochordum Mundi\" (2025).\n",
    "\n",
    "It is organized to mirror the research questions (RQ1–RQ3) and methods\n",
    "described in the manuscript.\n",
    "\n",
    "Sections:\n",
    "  0. Environment and configuration\n",
    "  1. Inputs and file layout\n",
    "  2. Helper functions\n",
    "  3. RQ1 – Monochord analysis (Terra–Aqua anomaly)\n",
    "  4. RQ1 controls – drafting noise from decorative columns\n",
    "  5. RQ3 – Textual musical over‑coding in Summum Bonum\n",
    "  6. Export key tables\n",
    "  7. Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ---\n",
    "# 0. Environment and configuration\n",
    "# ---\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "FIG_DIR = ROOT / \"figures\"\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Working directory:\", ROOT)\n",
    "print(\"Data directory:\", DATA_DIR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inputs and file layout\n",
    "\n",
    "Assumed file layout (relative to repository root):\n",
    "\n",
    "  data/monochord.tif                # high‑resolution scan of Monochordum Mundi\n",
    "  data/temple_of_music.tif          # negative control plate\n",
    "  data/decorative_columns/          # 50 decorative column crops\n",
    "  data/text/summum_bonum.txt        # normalized Latin text of Summum Bonum\n",
    "  data/text/controls/               # 10 control Latin texts (Frankfurt, 1610–1630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "MONOCHORD_PATH = DATA_DIR / \"monochord.tif\"\n",
    "TEMPLE_PATH = DATA_DIR / \"temple_of_music.tif\"\n",
    "COLUMNS_DIR = DATA_DIR / \"decorative_columns\"\n",
    "TEXT_SUMMUM = DATA_DIR / \"text\" / \"summum_bonum.txt\"\n",
    "TEXT_CONTROLS_DIR = DATA_DIR / \"text\" / \"controls\"\n",
    "\n",
    "print(\"Monochord image:      \", MONOCHORD_PATH)\n",
    "print(\"Temple of Music image:\", TEMPLE_PATH)\n",
    "print(\"Columns dir:          \", COLUMNS_DIR)\n",
    "print(\"Summum Bonum text:    \", TEXT_SUMMUM)\n",
    "print(\"Controls text dir:    \", TEXT_CONTROLS_DIR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2.1 Image loading and deskewing\n",
    "\n",
    "def load_grayscale(path: Path) -> np.ndarray:\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not load image: {path}\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def deskew_image(img: np.ndarray, max_angle: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Deskew using Hough transform around 0° within ±max_angle.\n",
    "\n",
    "    NOTE: This is intentionally lightweight. In the production pipeline you may\n",
    "    implement a more precise Hough‑based vertical‑line estimation. For the\n",
    "    1400‑DPI scans used in the paper, skew is typically very small.\n",
    "    \"\"\"\n",
    "    # Placeholder: assume near‑perfect alignment.\n",
    "    return img\n",
    "\n",
    "\n",
    "# 2.2 Edge detection and node extraction\n",
    "\n",
    "def detect_edges(img: np.ndarray, sigma: float = 1.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Canny edge detection with thresholds derived from the median intensity.\n",
    "\n",
    "    This mirrors the schema‑agnostic OMR approach: we want robust detection\n",
    "    of structural lines (monochord string and horizontal markers) without any\n",
    "    staff‑specific assumptions.\n",
    "    \"\"\"\n",
    "    v = np.median(img)\n",
    "    lower = int(max(0, (1.0 - 0.33) * v))\n",
    "    upper = int(min(255, (1.0 + 0.33) * v))\n",
    "    edges = cv2.Canny(img, lower, upper, L2gradient=True)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def extract_vertical_profile(edges: np.ndarray, x_center: int, half_width: int = 1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract a 1D vertical profile around the monochord string.\n",
    "\n",
    "    x_center should be chosen manually or from metadata, and corresponds to\n",
    "    the column where the vertical string runs.\n",
    "    \"\"\"\n",
    "    h, w = edges.shape\n",
    "    x0 = max(0, x_center - half_width)\n",
    "    x1 = min(w, x_center + half_width + 1)\n",
    "    column = edges[:, x0:x1].max(axis=1)\n",
    "    return column\n",
    "\n",
    "\n",
    "def detect_horizontal_intercepts(profile: np.ndarray, min_gap: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect y‑positions of horizontal lines intersecting the string.\n",
    "\n",
    "    This is a simple peak‑like detection over the vertical profile; it assumes\n",
    "    each node produces a local region of non‑zero edge responses separated by\n",
    "    at least min_gap pixels.\n",
    "    \"\"\"\n",
    "    ys = []\n",
    "    last_y = -min_gap\n",
    "    for y, val in enumerate(profile):\n",
    "        if val > 0 and (y - last_y) >= min_gap:\n",
    "            ys.append(y)\n",
    "            last_y = y\n",
    "    return np.array(ys)\n",
    "\n",
    "\n",
    "# 2.3 Ratio and cents computation\n",
    "\n",
    "def compute_nodes_dataframe(nodes_y: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given y‑coordinates of nodes along the string, compute normalized positions.\n",
    "\n",
    "    Assumes y=0 at top, increasing downward. The bottom node is Terra.\n",
    "    \"\"\"\n",
    "    if len(nodes_y) < 2:\n",
    "        raise ValueError(\"Need at least two nodes to compute intervals.\")\n",
    "\n",
    "    ys = np.sort(nodes_y)\n",
    "    length = ys[-1] - ys[0]\n",
    "    norm = (ys - ys[0]) / length\n",
    "\n",
    "    df = pd.DataFrame({\"y\": ys, \"norm\": norm})\n",
    "    return df\n",
    "\n",
    "\n",
    "def cents_from_ratio(r: float) -> float:\n",
    "    return 1200.0 * math.log2(r)\n",
    "\n",
    "\n",
    "# 2.4 Text utilities for RQ3\n",
    "\n",
    "def load_text(path: Path) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple normalization: keep only alphabetic characters, uppercase.\n",
    "\n",
    "    The real pipeline may add ligature normalization or accent stripping, but\n",
    "    this is sufficient to reproduce the A–G vs OTHER χ² logic.\n",
    "    \"\"\"\n",
    "    return \"\".join(ch.upper() for ch in text if ch.isalpha())\n",
    "\n",
    "\n",
    "def count_musical_letters(text: str) -> dict:\n",
    "    norm = normalize_text(text)\n",
    "    counts = {\"A_G\": 0, \"OTHER\": 0}\n",
    "    for ch in norm:\n",
    "        if ch in \"ABCDEFG\":\n",
    "            counts[\"A_G\"] += 1\n",
    "        else:\n",
    "            counts[\"OTHER\"] += 1\n",
    "    return counts"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RQ1 – Monochord analysis (Terra–Aqua anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.1 Load and preprocess the monochord image\n",
    "\n",
    "monochord = load_grayscale(MONOCHORD_PATH)\n",
    "mono_deskewed = deskew_image(monochord)\n",
    "mono_edges = detect_edges(mono_deskewed)\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.imshow(mono_edges, cmap=\"gray\")\n",
    "plt.title(\"Monochord – edge map (preview)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.2 Extract string profile and nodes\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT: Set x_center to the x‑coordinate of the monochord string in pixels.\n",
    "\n",
    "You can determine this by:\n",
    "  - Inspecting the edge image visually in an image viewer, or\n",
    "  - Storing it in a small JSON/CSV metadata file and loading it here.\n",
    "\"\"\"\n",
    "\n",
    "x_center = 1000  # TODO: replace with actual string x‑position from metadata\n",
    "profile = extract_vertical_profile(mono_edges, x_center=x_center, half_width=1)\n",
    "\n",
    "nodes_y = detect_horizontal_intercepts(profile, min_gap=20)\n",
    "print(\"Detected node y‑positions:\", nodes_y)\n",
    "\n",
    "nodes_df = compute_nodes_dataframe(nodes_y)\n",
    "print(nodes_df.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3.3 Label nodes and compute intervals\n",
    "\n",
    "\"\"\"\n",
    "Map vertical positions to semantic labels (Terra, Aqua, Aer, Ignis, Sol, etc.).\n",
    "The ordering and number of nodes must match the chosen plate and its annotation.\n",
    "\n",
    "For the canonical plate, you can either:\n",
    "  - Hard‑code the label ordering known from prior inspection, or\n",
    "  - Store it in a separate CSV and merge here.\n",
    "\"\"\"\n",
    "\n",
    "# Example placeholder label ordering (bottom to top):\n",
    "labels = [\n",
    "    \"Terra\",\n",
    "    \"Aqua\",\n",
    "    \"Aer\",\n",
    "    \"Ignis\",\n",
    "    \"Sol\",\n",
    "    \"Mars\",\n",
    "    \"Jupiter\",\n",
    "    \"Saturn\",\n",
    "    \"Firmamentum\",\n",
    "    \"Deus\",\n",
    "]\n",
    "\n",
    "nodes_df = nodes_df.sort_values(\"norm\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "if len(labels) == len(nodes_df):\n",
    "    nodes_df[\"label\"] = labels\n",
    "else:\n",
    "    nodes_df[\"label\"] = [f\"node_{i}\" for i in range(len(nodes_df))]\n",
    "\n",
    "print(nodes_df)\n",
    "\n",
    "# Compute adjacent intervals (string length segments and implied frequency ratios)\n",
    "\n",
    "interval_records = []\n",
    "for i in range(len(nodes_df) - 1):\n",
    "    lower = nodes_df.loc[i]\n",
    "    upper = nodes_df.loc[i + 1]\n",
    "\n",
    "    # Segment length as fraction of full string\n",
    "    length_segment = upper[\"norm\"] - lower[\"norm\"]\n",
    "    if length_segment <= 0:\n",
    "        continue\n",
    "\n",
    "    # For a monochord, frequency is inversely proportional to string length\n",
    "    freq_ratio = 1.0 / length_segment\n",
    "    cents = cents_from_ratio(freq_ratio)\n",
    "\n",
    "    interval_records.append(\n",
    "        {\n",
    "            \"from\": lower[\"label\"],\n",
    "            \"to\": upper[\"label\"],\n",
    "            \"length_segment\": float(length_segment),\n",
    "            \"freq_ratio\": float(freq_ratio),\n",
    "            \"cents\": float(cents),\n",
    "        }\n",
    "    )\n",
    "\n",
    "intervals_df = pd.DataFrame(interval_records)\n",
    "print(intervals_df)\n",
    "\n",
    "# TODO: Identify Terra–Aqua explicitly and compute z‑score vs 9:8 and σ_noise\n",
    "\n",
    "# Example: pick the Terra–Aqua row\n",
    "terra_aqua = intervals_df[\n",
    "    (intervals_df[\"from\"] == \"Terra\") & (intervals_df[\"to\"] == \"Aqua\")\n",
    "]\n",
    "print(\"Terra–Aqua interval (raw):\")\n",
    "print(terra_aqua)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RQ1 controls – drafting noise from decorative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "column_paths = []\n",
    "if COLUMNS_DIR.is_dir():\n",
    "    for name in sorted(os.listdir(COLUMNS_DIR)):\n",
    "        if name.lower().endswith((\".tif\", \".png\", \".jpg\", \".jpeg\")):\n",
    "            column_paths.append(COLUMNS_DIR / name)\n",
    "\n",
    "print(f\"Found {len(column_paths)} column images.\")\n",
    "\n",
    "symmetry_devs = []\n",
    "for path in column_paths:\n",
    "    img = load_grayscale(path)\n",
    "    h, w = img.shape\n",
    "    left = img[:, : w // 2]\n",
    "    right = img[:, w - w // 2 :]\n",
    "    right_flipped = np.fliplr(right)\n",
    "    diff = left.astype(float) - right_flipped.astype(float)\n",
    "    rms = np.sqrt(np.mean(diff ** 2))\n",
    "    symmetry_devs.append(rms)\n",
    "\n",
    "symmetry_devs = np.array(symmetry_devs)\n",
    "\n",
    "if len(symmetry_devs) > 0:\n",
    "    print(\"Symmetry deviation stats (arbitrary units):\")\n",
    "    print(\"  mean =\", symmetry_devs.mean())\n",
    "    print(\"  std  =\", symmetry_devs.std(ddof=1))\n",
    "\n",
    "# NOTE: In the production analysis, you convert these deviations into an\n",
    "# equivalent σ in cents (σ_noise ≈ 21 cents) using the calibration described\n",
    "# in the manuscript."
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RQ3 – Textual musical over‑coding in Summum Bonum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 5.1 Load Summum Bonum and control texts\n",
    "\n",
    "summum_text = load_text(TEXT_SUMMUM)\n",
    "control_texts = []\n",
    "\n",
    "if TEXT_CONTROLS_DIR.is_dir():\n",
    "    for name in sorted(os.listdir(TEXT_CONTROLS_DIR)):\n",
    "        if name.lower().endswith(\".txt\"):\n",
    "            control_texts.append(load_text(TEXT_CONTROLS_DIR / name))\n",
    "\n",
    "print(f\"Loaded 1 Summum Bonum text and {len(control_texts)} control texts.\")\n",
    "\n",
    "# 5.2 Count musical letters and run χ² test\n",
    "\n",
    "summum_counts = count_musical_letters(summum_text)\n",
    "\n",
    "control_counts = {\"A_G\": 0, \"OTHER\": 0}\n",
    "for txt in control_texts:\n",
    "    c = count_musical_letters(txt)\n",
    "    control_counts[\"A_G\"] += c[\"A_G\"]\n",
    "    control_counts[\"OTHER\"] += c[\"OTHER\"]\n",
    "\n",
    "print(\"Summum counts:\", summum_counts)\n",
    "print(\"Control counts (pooled):\", control_counts)\n",
    "\n",
    "# Contingency table: rows = (Summum, Controls), cols = (A–G, OTHER)\n",
    "obs = np.array(\n",
    "    [\n",
    "        [summum_counts[\"A_G\"], summum_counts[\"OTHER\"]],\n",
    "        [control_counts[\"A_G\"], control_counts[\"OTHER\"]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "chi2, p, dof, expected = stats.chi2_contingency(obs)\n",
    "print(\"χ² =\", chi2, \"p =\", p, \"dof =\", dof)\n",
    "print(\"Expected counts:\")\n",
    "print(expected)\n",
    "\n",
    "# Odds ratio\n",
    "odds_summum = summum_counts[\"A_G\"] / max(1, summum_counts[\"OTHER\"])\n",
    "odds_control = control_counts[\"A_G\"] / max(1, control_counts[\"OTHER\"])\n",
    "odds_ratio = odds_summum / odds_control if odds_control > 0 else float(\"inf\")\n",
    "print(\"Odds ratio (Summum vs controls):\", odds_ratio)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export key tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "nodes_out = DATA_DIR / \"monochord_nodes.csv\"\n",
    "intervals_out = DATA_DIR / \"monochord_intervals.csv\"\n",
    "\n",
    "nodes_df.to_csv(nodes_out, index=False)\n",
    "intervals_df.to_csv(intervals_out, index=False)\n",
    "\n",
    "print(\"Saved:\", nodes_out)\n",
    "print(\"Saved:\", intervals_out)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Notes\n",
    "\n",
    "To match the paper exactly, you should:\n",
    "\n",
    "  - Set `x_center` and `labels` to the values used in the arXiv analysis\n",
    "    (or load them from a small metadata file).\n",
    "  - Replace the simple symmetry RMS with the calibrated σ_noise ≈ 21 cents\n",
    "    derived from decorative columns, as described in Section 3.3.\n",
    "  - Plug the measured Terra–Aqua cents value and σ_noise into a z‑score\n",
    "    and coincidence calculation matching the reported >4σ anomaly and\n",
    "    p ≈ 0.004 coincidence probability.\n",
    "  - Optionally, add plots of interval distributions and control histograms\n",
    "    to mirror the manuscript figures.\n",
    "\n",
    "This notebook is intended as the main reproducibility entry point for the\n",
    "project (Tag v1.0.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
